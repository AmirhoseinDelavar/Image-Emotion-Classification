# -*- coding: utf-8 -*-
"""CI-HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QZ7btUaKCqDwbjWnIUNghselW40J5Txn

# download and uzip
"""

pip install gdown

!gdown https://drive.google.com/u/0/uc?id=1FUXSmzQXl4lUjHE6vDvXPtAmuo_azcIo

from zipfile import ZipFile
  
# specifying the zip file name
file_name = "/content/train.zip"
  
# opening the zip file in READ mode
with ZipFile(file_name, 'r') as zip:
    # printing all the contents of the zip file
    zip.printdir()
    # extracting all the files
    print('Extracting all the files now...')
    zip.extractall()
    print('Done!')

"""# preprocessing

## read
"""

import pandas as pd
import numpy as np

df = pd.read_csv('train.csv')

df.head()

"""## read the string with numpy"""

arr = np.fromstring(df.values[0][1], dtype=int, sep=' ')

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
imgplot = plt.imshow(arr.reshape((48,48)))

"""## count class types"""

df.emotion.value_counts()

"""# preprocess and read images as image"""

def convert_to_img(x):
  arr = np.fromstring(x[1], dtype=int, sep=' ')
  return arr.reshape((48,48))

df['image'] = df.apply(lambda x: convert_to_img(x),axis=1)

"""# save images in dataloader format"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    df.image.values, df.emotion.values, test_size=0.33, random_state=42)

np.unique(y_train,return_counts=True)

np.unique(y_test,return_counts=True)

"""## make a custom dataset to fetch images and properly convert them to tensors"""

import torch
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader,
import numpy as np
from PIL import Image

mean = np.array([0.485])
std = np.array([0.229])


class MyDataset(Dataset):
    def __init__(self, data, targets, transform=None):
        self.data = data
        self.targets = torch.LongTensor(targets)
        self.transform = transform
        
    def __getitem__(self, index):
        x = self.data[index]
        y = self.targets[index]
        
        # transform them
        if self.transform:
            x = Image.fromarray(np.uint8(self.data[index]), 'L')
            x = self.transform(x)
        
        return x, y
    
    def __len__(self):
        return len(self.data)


transform = transforms.Compose([transforms.ToTensor()])
dataset_train = MyDataset(X_train, y_train, transform=transform)
dataset_val = MyDataset(X_test, y_test, transform=transform)
# make dataloader object
dataloaders = {'train':DataLoader(dataset_train, batch_size=1, shuffle=True),
               'val':DataLoader(dataset_val, batch_size=1)}
dataset_sizes = {'train':len(dataset_train),
               'val':len(dataset_val)}
class_names = np.unique(y_train)

# print a sample of image in array format
Image.fromarray(np.uint8(X_train[0]), 'L')

"""# Train

## use gpu for best performace

### on each loop of train method, for training mode the gradient calculation is enabled, backprop is calculated and steper makes on step, for validation mode we just get forward results and call loss fuction with the loss is minimum we copy the model, so the last model is the best model scored on validation score
"""

sets = ['train','val']
device = 'cuda'

def train_model(model, criterion, optimizer, schedular, num_epochs=25):
    since = time.time()

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print(f'Epoch {epoch}/{num_epochs-1}')
        print('-'*10)

        # Each epoch has a training and validation phase
        for phase in sets:
            if phase == 'train':
                model.train()
            else:
                model.eval()
            
            running_loss = 0.0
            running_corrects = 0

            # Iterate over data
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    # print(outputs)
                    # print(labels)
                    loss = criterion(outputs, labels)
                    # backward + optimize if training
                    if phase == 'train':
                        optimizer.zero_grad()
                        loss.backward()
                        optimizer.step()
                # statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
            if phase == 'train':
                schedular.step()

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            print(f'{phase} Loss: {epoch_loss} Acc: {epoch_acc}')

            # deep copy the model
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
        print()
    time_elsp = time.time() - since
    print(f'Training complete in {time_elsp//60} mins {time_elsp%60}')
    print(f'Best val Acc: {best_acc}')

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model

"""### model composed of two cov2d and flattening with 3 dense layers at end

### to avoid useless caculation the pool2 layer is bigger
"""

import torch.nn as nn
class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 4, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(4, 16, 5)
        self.pool2 = nn.MaxPool2d(4, 4)

        self.fc1 = nn.Linear(256, 64)
        self.fc2 = nn.Linear(64, 16)
        self.fc3 = nn.Linear(16, len(class_names))


    def forward(self,x):
        
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool2(F.relu(self.conv2(x)))
        # print(x.shape)
        x = x.view(-1, 256)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

"""## put model to gpu, and setup loss function, and optimizer"""

model = ConvNet().to('cuda')

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

"""## setup scheduler to multiply gamma to learning rate every 7 iterations

## As the validation gets near and above 50 in the last run the model is nearly solid because the state-of-art results is near and above 50 for a multi-class image classification
"""

from torch.optim import lr_scheduler
import torch.nn.functional as F
import time
import os
import copy
step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=100)